# -*- coding: utf-8 -*-
"""Model_Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b7mCdXre5pKHz7Y9cz1_-fDnvZtwTBqz
"""



# import pandas as pd
# import tensorflow as tf
# import tensorflow_hub as hub
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import LabelEncoder, OneHotEncoder
# from tensorflow.keras.layers import LSTM, Dense, Input
# from tensorflow.keras.models import Model
# from tensorflow.keras.optimizers import Adam

# # Load the dataset
# data = pd.read_csv("https://raw.githubusercontent.com/adi31891/capstone-C241-PR565/f8a43716e964aa3bc19b1de1d1947aa550dd6bb9/data/capstone_new_dataset.csv")

# # Extract text and labels
# texts = data['text'].astype(str).values
# labels = data['sentiment'].astype(str).values

# # Encode labels to integers
# label_encoder = LabelEncoder()
# integer_encoded_labels = label_encoder.fit_transform(labels)

# # One-hot encode the integer labels
# onehot_encoder = OneHotEncoder(sparse=False)
# integer_encoded_labels = integer_encoded_labels.reshape(len(integer_encoded_labels), 1)
# onehot_encoded_labels = onehot_encoder.fit_transform(integer_encoded_labels)

# # Split the dataset into training and testing sets
# X_train, X_test, y_train, y_test = train_test_split(texts, onehot_encoded_labels, test_size=0.2, random_state=42)

# # Load the Universal Sentence Encoder
# embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")

# # Function to create embeddings for a given list of texts in batches
# def get_embeddings_in_batches(texts, batch_size=256):
#     embeddings = []
#     for i in range(0, len(texts), batch_size):
#         batch_texts = texts[i:i+batch_size]
#         batch_embeddings = embed(batch_texts)
#         embeddings.append(batch_embeddings)
#     return tf.concat(embeddings, axis=0)

# # Create embeddings for train and test sets in batches
# train_embeddings = get_embeddings_in_batches(X_train)
# test_embeddings = get_embeddings_in_batches(X_test)

# # Define the LSTM model
# input_shape = train_embeddings.shape[1]
# inputs = Input(shape=(input_shape,), dtype=tf.float32)
# x = tf.expand_dims(inputs, -1)
# x = LSTM(64)(x)
# outputs = Dense(3, activation='softmax')(x)

# model = Model(inputs, outputs)
# model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# # Train the model
# model.fit(train_embeddings, y_train, epochs=10, batch_size=32, validation_data=(test_embeddings, y_test))

# # Function to predict sentiment
# def predict_sentiment(text):
#     embedding = embed([text])
#     prediction = model.predict(embedding)
#     sentiment_index = tf.argmax(prediction[0]).numpy()
#     sentiment = label_encoder.inverse_transform([sentiment_index])[0]
#     return sentiment

# # Example usage with Gen AI response
# def generate_response(sentiment):
#     if sentiment == 'good':
#         return "I'm glad to hear that! Keep up the positive vibes."
#     elif sentiment == 'bad':
#         return "I'm sorry to hear that. Remember, it's okay to feel this way sometimes. Can I help with something?"
#     elif sentiment == 'neutral':
#         return "Thanks for sharing your feelings."
#     else:
#         return "Thanks for sharing your feelings."

# # Predict sentiment and generate a response
# user_message = "I feel so anxious about my exams."
# predicted_sentiment = predict_sentiment(user_message)
# response = generate_response(predicted_sentiment)

# print(f"User message: {user_message}")
# print(f"Predicted sentiment: {predicted_sentiment}")
# print(f"Response: {response}")

# Instal versi TensorFlow dan transformers yang kompatibel
!pip install tensorflow==2.12.0
!pip install transformers==4.30.2

# import pandas as pd
# import tensorflow as tf
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import LabelEncoder, OneHotEncoder
# from transformers import BertTokenizer, TFBertForSequenceClassification

# # Load the dataset
# data = pd.read_csv("https://raw.githubusercontent.com/adi31891/capstone-C241-PR565/f8a43716e964aa3bc19b1de1d1947aa550dd6bb9/data/capstone_new_dataset.csv")

# # Extract text and labels
# texts = data['text'].astype(str).values
# labels = data['sentiment'].astype(str).values

# # Encode labels to integers
# label_encoder = LabelEncoder()
# integer_encoded_labels = label_encoder.fit_transform(labels)

# # One-hot encode the integer labels
# onehot_encoder = OneHotEncoder(sparse_output=False)
# integer_encoded_labels = integer_encoded_labels.reshape(len(integer_encoded_labels), 1)
# onehot_encoded_labels = onehot_encoder.fit_transform(integer_encoded_labels)

# # Split the dataset into training and testing sets
# X_train, X_test, y_train, y_test = train_test_split(texts, onehot_encoded_labels, test_size=0.2, random_state=42)

# # Load BERT tokenizer and model from Hugging Face
# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
# bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)

# # Tokenize the input texts
# def tokenize_texts(texts, tokenizer, max_length=128):
#     return tokenizer(
#         texts.tolist(),
#         max_length=max_length,
#         truncation=True,
#         padding=True,
#         return_tensors='tf'
#     )

# X_train_tokens = tokenize_texts(X_train, tokenizer)
# X_test_tokens = tokenize_texts(X_test, tokenizer)

# # Compile the model
# optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)
# loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
# metrics = ['accuracy']
# bert_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

# # Train the model
# bert_model.fit(
#     [X_train_tokens['input_ids'], X_train_tokens['attention_mask']],
#     y_train,
#     epochs=5,
#     batch_size=16,
#     validation_data=([X_test_tokens['input_ids'], X_test_tokens['attention_mask']], y_test)
# )

# # Function to predict sentiment
# def predict_sentiment(text):
#     tokens = tokenize_texts([text], tokenizer)
#     prediction = bert_model.predict([tokens['input_ids'], tokens['attention_mask']])
#     sentiment_index = tf.argmax(prediction.logits[0]).numpy()
#     sentiment = label_encoder.inverse_transform([sentiment_index])[0]
#     return sentiment

# # Example usage with Gen AI response
# def generate_response(sentiment):
#     if sentiment == 'good':
#         return "I'm glad to hear that! Keep up the positive vibes."
#     elif sentiment == 'bad':
#         return "I'm sorry to hear that. Remember, it's okay to feel this way sometimes. Can I help with something?"
#     elif sentiment == 'neutral':
#         return "Thanks for sharing your feelings."
#     else:
#         return "Thanks for sharing your feelings."

# # Predict sentiment and generate a response
# user_message = "I feel so anxious about my exams."
# predicted_sentiment = predict_sentiment(user_message)
# response = generate_response(predicted_sentiment)

# print(f"User message: {user_message}")
# print(f"Predicted sentiment: {predicted_sentiment}")
# print(f"Response: {response}")

import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from transformers import BertTokenizer, TFBertForSequenceClassification

# Load the dataset
data = pd.read_csv("https://raw.githubusercontent.com/adi31891/capstone-C241-PR565/f8a43716e964aa3bc19b1de1d1947aa550dd6bb9/data/capstone_new_dataset.csv")

# Extract text and labels
texts = data['text'].astype(str).values
labels = data['sentiment'].astype(str).values

# Encode labels to integers
label_encoder = LabelEncoder()
integer_encoded_labels = label_encoder.fit_transform(labels)

# One-hot encode the integer labels
onehot_encoder = OneHotEncoder(sparse_output=False)
integer_encoded_labels = integer_encoded_labels.reshape(len(integer_encoded_labels), 1)
onehot_encoded_labels = onehot_encoder.fit_transform(integer_encoded_labels)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(texts, onehot_encoded_labels, test_size=0.2, random_state=42)

# Load BERT tokenizer and model from Hugging Face
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)

# Tokenize the input texts
def tokenize_texts(texts, tokenizer, max_length=128):
    return tokenizer(
        texts.tolist(),
        max_length=max_length,
        truncation=True,
        padding=True,
        return_tensors='tf'
    )

X_train_tokens = tokenize_texts(X_train, tokenizer)
X_test_tokens = tokenize_texts(X_test, tokenizer)

# Compile the model
optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)
loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
metrics = ['accuracy']
bert_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

# Train the model
bert_model.fit(
    [X_train_tokens['input_ids'], X_train_tokens['attention_mask']],
    y_train,
    epochs=5,
    batch_size=16,
    validation_data=([X_test_tokens['input_ids'], X_test_tokens['attention_mask']], y_test)
)

# Simpan bobot model ke file .bin
bert_model.save_weights('bert_model_weights.bin')

# Fungsi untuk memuat bobot model
def load_model_weights(model, weights_path):
    model.load_weights(weights_path)
    return model

# Function to predict sentiment
def predict_sentiment(text):
    tokens = tokenize_texts([text], tokenizer)
    prediction = bert_model.predict([tokens['input_ids'], tokens['attention_mask']])
    sentiment_index = tf.argmax(prediction.logits[0]).numpy()
    sentiment = label_encoder.inverse_transform([sentiment_index])[0]
    return sentiment

# Example usage with Gen AI response
def generate_response(sentiment):
    if sentiment == 'good':
        return "I'm glad to hear that! Keep up the positive vibes."
    elif sentiment == 'bad':
        return "I'm sorry to hear that. Remember, it's okay to feel this way sometimes. Can I help with something?"
    elif sentiment == 'neutral':
        return "Thanks for sharing your feelings."
    else:
        return "Thanks for sharing your feelings."

# Predict sentiment and generate a response
user_message = "I feel so anxious about my exams."
predicted_sentiment = predict_sentiment(user_message)
response = generate_response(predicted_sentiment)

print(f"User message: {user_message}")
print(f"Predicted sentiment: {predicted_sentiment}")
print(f"Response: {response}")

# # Import necessary libraries
# import pandas as pd
# import tensorflow as tf
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import LabelEncoder, OneHotEncoder
# from transformers import BertTokenizer, TFBertForSequenceClassification

# # Enable TPU
# try:
#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
#     print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])
#     tf.config.experimental_connect_to_cluster(tpu)
#     tf.tpu.experimental.initialize_tpu_system(tpu)
#     strategy = tf.distribute.experimental.TPUStrategy(tpu)
# except ValueError:
#     strategy = tf.distribute.get_strategy()

# # Load the dataset
# data = pd.read_csv("https://raw.githubusercontent.com/adi31891/capstone-C241-PR565/f8a43716e964aa3bc19b1de1d1947aa550dd6bb9/data/capstone_new_dataset.csv")

# # Extract text and labels
# texts = data['text'].astype(str).values
# labels = data['sentiment'].astype(str).values

# # Encode labels to integers
# label_encoder = LabelEncoder()
# integer_encoded_labels = label_encoder.fit_transform(labels)

# # One-hot encode the integer labels
# onehot_encoder = OneHotEncoder(sparse_output=False)
# integer_encoded_labels = integer_encoded_labels.reshape(len(integer_encoded_labels), 1)
# onehot_encoded_labels = onehot_encoder.fit_transform(integer_encoded_labels)

# # Split the dataset into training and testing sets
# X_train, X_test, y_train, y_test = train_test_split(texts, onehot_encoded_labels, test_size=0.2, random_state=42)

# # Load BERT tokenizer and model from Hugging Face
# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# # Tokenize the input texts
# def tokenize_texts(texts, tokenizer, max_length=128):
#     return tokenizer(
#         texts.tolist(),
#         max_length=max_length,
#         truncation=True,
#         padding=True,
#         return_tensors='tf'
#     )

# X_train_tokens = tokenize_texts(X_train, tokenizer)
# X_test_tokens = tokenize_texts(X_test, tokenizer)

# # Compile and train the model inside the TPU strategy scope
# with strategy.scope():
#     bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)
#     optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)
#     loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
#     metrics = ['accuracy']
#     bert_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

#     # Train the model
#     bert_model.fit(
#         [X_train_tokens['input_ids'], X_train_tokens['attention_mask']],
#         y_train,
#         epochs=5,
#         batch_size=16,  # Adjust batch size as needed
#         validation_data=([X_test_tokens['input_ids'], X_test_tokens['attention_mask']], y_test),
#         callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]
#     )

#     # Save the model as .h5
#     bert_model.save_weights('bert_model.h5')

# # Function to predict sentiment
# def predict_sentiment(text):
#     tokens = tokenize_texts([text], tokenizer)
#     prediction = bert_model.predict([tokens['input_ids'], tokens['attention_mask']])
#     sentiment_index = tf.argmax(prediction.logits[0]).numpy()
#     sentiment = label_encoder.inverse_transform([sentiment_index])[0]
#     return sentiment

# # Example usage with Gen AI response
# def generate_response(sentiment):
#     if sentiment == 'good':
#         return "I'm glad to hear that! Keep up the positive vibes."
#     elif sentiment == 'bad':
#         return "I'm sorry to hear that. Remember, it's okay to feel this way sometimes. Can I help with something?"
#     elif sentiment == 'neutral':
#         return "Thanks for sharing your feelings."
#     else:
#         return "Thanks for sharing your feelings."

# # Predict sentiment and generate a response
# user_message = "I feel so anxious about my exams."
# predicted_sentiment = predict_sentiment(user_message)
# response = generate_response(predicted_sentiment)

# print(f"User message: {user_message}")
# print(f"Predicted sentiment: {predicted_sentiment}")
# print(f"Response: {response}")

