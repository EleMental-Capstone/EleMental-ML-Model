{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EleMental-Capstone/EleMental-Machine-Learning/blob/models_trial/LSTM_Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwAFIVTWfBPK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6vQ2qXfDmZc",
        "outputId": "8491fc0a-5c22-4040-f626-dc7fc526e78b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input, BatchNormalization, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/adi31891/capstone-C241-PR565/f8a43716e964aa3bc19b1de1d1947aa550dd6bb9/data/capstone_new_dataset.csv\")\n",
        "\n",
        "# Extract text and labels\n",
        "texts = data['text'].astype(str).values\n",
        "labels = data['sentiment'].astype(str).values\n",
        "\n",
        "# Encode labels to integers\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# One-hot encode the integer labels\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded_labels = integer_encoded_labels.reshape(len(integer_encoded_labels), 1)\n",
        "onehot_encoded_labels = onehot_encoder.fit_transform(integer_encoded_labels)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, onehot_encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the Universal Sentence Encoder\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "\n",
        "# Function to create embeddings for a given list of texts in batches\n",
        "def get_embeddings_in_batches(texts, batch_size=256):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        batch_embeddings = embed(batch_texts)\n",
        "        embeddings.append(batch_embeddings)\n",
        "    return tf.concat(embeddings, axis=0)\n",
        "\n",
        "# Create embeddings for train and test sets in batches\n",
        "train_embeddings = get_embeddings_in_batches(X_train)\n",
        "test_embeddings = get_embeddings_in_batches(X_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuAnfpfrhsIP",
        "outputId": "32651b0e-badc-4cce-8af6-356785adc381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "5743/5743 [==============================] - 204s 34ms/step - loss: 1.0188 - accuracy: 0.5025 - val_loss: 1.1476 - val_accuracy: 0.3520\n",
            "Epoch 2/20\n",
            "5743/5743 [==============================] - 195s 34ms/step - loss: 1.0027 - accuracy: 0.5165 - val_loss: 1.1402 - val_accuracy: 0.4924\n",
            "Epoch 3/20\n",
            "5743/5743 [==============================] - 196s 34ms/step - loss: 0.9886 - accuracy: 0.5276 - val_loss: 1.1132 - val_accuracy: 0.4948\n",
            "Epoch 4/20\n",
            "5743/5743 [==============================] - 195s 34ms/step - loss: 0.9783 - accuracy: 0.5343 - val_loss: 1.0803 - val_accuracy: 0.4461\n",
            "Epoch 5/20\n",
            "5743/5743 [==============================] - 196s 34ms/step - loss: 0.9751 - accuracy: 0.5360 - val_loss: 1.0927 - val_accuracy: 0.4906\n",
            "Epoch 6/20\n",
            "5743/5743 [==============================] - 198s 35ms/step - loss: 0.9678 - accuracy: 0.5412 - val_loss: 1.0135 - val_accuracy: 0.5137\n",
            "Epoch 7/20\n",
            "5743/5743 [==============================] - 198s 35ms/step - loss: 0.9655 - accuracy: 0.5425 - val_loss: 1.1166 - val_accuracy: 0.4939\n",
            "Epoch 8/20\n",
            "5743/5743 [==============================] - 195s 34ms/step - loss: 0.9601 - accuracy: 0.5464 - val_loss: 1.0427 - val_accuracy: 0.4506\n",
            "Epoch 9/20\n",
            "5743/5743 [==============================] - 194s 34ms/step - loss: 0.9536 - accuracy: 0.5499 - val_loss: 0.9958 - val_accuracy: 0.5218\n",
            "Epoch 10/20\n",
            "5743/5743 [==============================] - 193s 34ms/step - loss: 0.9488 - accuracy: 0.5524 - val_loss: 1.0021 - val_accuracy: 0.5264\n",
            "Epoch 11/20\n",
            "5743/5743 [==============================] - 194s 34ms/step - loss: 0.9465 - accuracy: 0.5547 - val_loss: 0.9830 - val_accuracy: 0.5204\n",
            "Epoch 12/20\n",
            "5743/5743 [==============================] - 196s 34ms/step - loss: 0.9431 - accuracy: 0.5557 - val_loss: 1.0222 - val_accuracy: 0.5185\n",
            "Epoch 13/20\n",
            "5743/5743 [==============================] - 196s 34ms/step - loss: 0.9443 - accuracy: 0.5542 - val_loss: 1.6188 - val_accuracy: 0.4901\n",
            "Epoch 14/20\n",
            "5743/5743 [==============================] - 195s 34ms/step - loss: 0.9388 - accuracy: 0.5581 - val_loss: 0.9879 - val_accuracy: 0.5248\n",
            "Epoch 15/20\n",
            "5743/5743 [==============================] - 197s 34ms/step - loss: 0.9383 - accuracy: 0.5588 - val_loss: 0.9565 - val_accuracy: 0.5441\n",
            "Epoch 16/20\n",
            "5743/5743 [==============================] - 195s 34ms/step - loss: 0.9346 - accuracy: 0.5608 - val_loss: 0.9673 - val_accuracy: 0.5436\n",
            "Epoch 17/20\n",
            "5743/5743 [==============================] - 192s 33ms/step - loss: 0.9340 - accuracy: 0.5609 - val_loss: 0.9816 - val_accuracy: 0.5375\n",
            "Epoch 18/20\n",
            "5743/5743 [==============================] - 191s 33ms/step - loss: 0.9321 - accuracy: 0.5613 - val_loss: 1.0415 - val_accuracy: 0.4866\n",
            "Epoch 19/20\n",
            "5743/5743 [==============================] - 190s 33ms/step - loss: 0.9308 - accuracy: 0.5624 - val_loss: 1.1800 - val_accuracy: 0.4968\n",
            "Epoch 20/20\n",
            "5743/5743 [==============================] - 190s 33ms/step - loss: 0.9311 - accuracy: 0.5636 - val_loss: 0.9743 - val_accuracy: 0.5291\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b112037f940>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Define the LSTM model\n",
        "input_shape = train_embeddings.shape[1]\n",
        "inputs = Input(shape=(input_shape,), dtype=tf.float32)\n",
        "x = tf.expand_dims(inputs, -1)\n",
        "x = LSTM(32, return_sequences=True)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LSTM(16)(x)\n",
        "# x = Dropout(0.2)(x)\n",
        "x = BatchNormalization()(x)\n",
        "outputs = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_embeddings, y_train, epochs=20, batch_size=32, validation_data=(test_embeddings, y_test))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lylpNOThxNE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "534634a1-ffe9-4599-d544-5b7d3f1e8c8f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ffb914a33624>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save the model architecture as JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lstm_model.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# Save the model architecture as JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"lstm_model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# Save the model weights\n",
        "model.save_weights(\"lstm_model_weights.h5\")\n",
        "\n",
        "# Save Universal Sentence Encoder weights to .bin file\n",
        "# Get all variables from the module\n",
        "variables = embed.variables\n",
        "with open('use_model_weights.bin', 'wb') as f:\n",
        "    for var in variables:\n",
        "        np.array(var).tofile(f)\n",
        "\n",
        "# Function to predict sentiment\n",
        "def predict_sentiment(text):\n",
        "    embedding = embed([text])\n",
        "    prediction = model.predict(embedding)\n",
        "    sentiment_index = tf.argmax(prediction[0]).numpy()\n",
        "    sentiment = label_encoder.inverse_transform([sentiment_index])[0]\n",
        "    return sentiment\n",
        "\n",
        "# Example usage with Gen AI response\n",
        "def generate_response(sentiment):\n",
        "    if sentiment == 'good':\n",
        "        return \"I'm glad to hear that! Keep up the positive vibes.\"\n",
        "    elif sentiment == 'bad':\n",
        "        return \"I'm sorry to hear that. Remember, it's okay to feel this way sometimes. Can I help with something?\"\n",
        "    elif sentiment == 'neutral':\n",
        "        return \"Thanks for sharing your feelings.\"\n",
        "    else:\n",
        "        return \"Thanks for sharing your feelings.\"\n",
        "\n",
        "# Predict sentiment and generate a response\n",
        "user_message = \"I feel so anxious about my exams.\"\n",
        "predicted_sentiment = predict_sentiment(user_message)\n",
        "response = generate_response(predicted_sentiment)\n",
        "\n",
        "print(f\"User message: {user_message}\")\n",
        "print(f\"Predicted sentiment: {predicted_sentiment}\")\n",
        "print(f\"Response: {response}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}